{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e38128d-4a3e-4685-992a-effad72ecc1d",
   "metadata": {},
   "source": [
    "# Data Science HW4\n",
    "### Report: https://hackmd.io/@ohmygod0193/HJgdf3k7R\n",
    "torch.nn tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1d2033-adff-42b0-afd9-9de5a4f56c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # change device number if there exists more than one gpu on your platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddf7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bf7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a17bb97-639a-48f0-850e-37fb834fb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5small_TextSummarization/\" # released full model path\n",
    "TK_ckpt = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TK_ckpt)  # use tokeniozer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811efa96-affc-44f8-b161-1f153743472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3671d0a3-1fc8-4de4-b256-eaad6941b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22dc7826-e1e5-4e1d-b092-9f8ac7b7d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db040045-043b-404f-b030-c9a6debd8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c972f9ce-68cf-4f0e-9468-6af941689873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15159/15159 [00:06<00:00, 2367.51 examples/s]\n",
      "Map: 100%|██████████| 3790/3790 [00:01<00:00, 2118.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "billsum = (load_dataset('billsum', split='train').train_test_split(test_size=0.2))\n",
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f0686-dd7f-48c3-a12f-5f87070cab23",
   "metadata": {},
   "source": [
    "## Ratio of non-zero parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89716f50-5735-4f7f-8c6d-9712717efd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_param_ratio(model):\n",
    "    num_param = 0\n",
    "    for param in model.parameters():\n",
    "        num_param += param.numel()\n",
    "    num_mask = 0\n",
    "    for name, param in model.named_buffers():\n",
    "        if \"mask\" in name:\n",
    "            num_mask += int((param == 0).sum())\n",
    "    print(num_param/1e6,'M')\n",
    "    print((num_param - num_mask) / num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225407cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "from transformers.models.t5.modeling_t5 import T5LayerSelfAttention, T5LayerCrossAttention, T5LayerFF\n",
    "\n",
    "def puring(pruned_model,parameters_to_prune,amounts_to_prune):\n",
    "    for name, module in pruned_model.named_modules():\n",
    "        part = name.split('.')[0]+'_'\n",
    "        if isinstance(module, T5LayerSelfAttention):\n",
    "            for name2, layer in module.named_modules():\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    parameters_to_prune[part+'T5LayerSelfAttention'].append((layer, 'weight'))\n",
    "        elif isinstance(module, T5LayerCrossAttention):\n",
    "            for name2, layer in module.named_modules():\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    parameters_to_prune[part+'T5LayerCrossAttention'].append((layer, 'weight'))\n",
    "        elif isinstance(module, T5LayerFF):\n",
    "            for name2, layer in module.named_modules():\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    parameters_to_prune[part+'T5LayerFF'].append((layer, 'weight'))\n",
    "        elif isinstance(module, torch.nn.Linear) and name==\"lm_head\":\n",
    "            parameters_to_prune['lm_head'].append((module, 'weight'))\n",
    "    for name,amount in zip(parameters_to_prune,amounts_to_prune):\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune[name],\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=amount,\n",
    "        )\n",
    "    show_param_ratio(pruned_model)\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a7e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pruned_model(pruned_model,output_dir,tokenized_billsum,tokenizer,data_collator,compute_metrics,epochs):\n",
    "    pruned_model.to(device)\n",
    "    pruned_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=epochs,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        fp16=True,\n",
    "        logging_steps=10000,\n",
    "        predict_with_generate=True,\n",
    "        #metric_for_best_model=\"rougel\",\n",
    "        #load_best_model_at_end=True,\n",
    "        save_steps=10000,\n",
    "        #do_eval=False,\n",
    "        eval_steps=10000  # Add this line to evaluate every 5 epochs\n",
    "    )\n",
    "    pruned_trainer = Seq2SeqTrainer(\n",
    "        model=pruned_model,\n",
    "        args=pruned_training_args,\n",
    "        train_dataset=tokenized_billsum['train'],\n",
    "        eval_dataset=tokenized_billsum[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    pruned_trainer.train()\n",
    "    return pruned_model, pruned_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "def load_pruned_model(model_name):\n",
    "    pruned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    import torch\n",
    "    import torch.nn.utils.prune as prune\n",
    "    import torch.nn.utils.prune as prune\n",
    "    from transformers.models.t5.modeling_t5 import T5LayerSelfAttention, T5LayerCrossAttention, T5LayerFF\n",
    "    # Apply prune.identity to the layers that were pruned\n",
    "    for module in pruned_model.modules():\n",
    "        if isinstance(module, torch.nn.Linear):  # Check the layer type as per your model's pruned layers\n",
    "            prune.identity(module, 'weight')\n",
    "    pruned_model.load_state_dict(torch.load(model_name+'/model_state_dict.pth'))\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f75a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "show_param_ratio(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4daf36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.506624 M\n",
      "0.772869496073686\n",
      "60.506624 M\n",
      "0.6055597978826253\n",
      "60.506624 M\n",
      "0.48116442589822894\n",
      "60.506624 M\n",
      "0.38770331988775314\n",
      "60.506624 M\n",
      "0.31666774203102127\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    parameters_to_prune = {\n",
    "        'encoder_T5LayerSelfAttention':[], \n",
    "        'encoder_T5LayerFF':[],\n",
    "        'decoder_T5LayerSelfAttention':[], \n",
    "        'decoder_T5LayerCrossAttention':[],\n",
    "        'decoder_T5LayerFF':[],\n",
    "        'lm_head':[],\n",
    "    }\n",
    "    amounts_to_prune = [\n",
    "        0.1,\n",
    "        0.1,\n",
    "        0.3,\n",
    "        0.2,\n",
    "        0.3,\n",
    "        0.3,\n",
    "    ]\n",
    "    pruned_model = puring(model,parameters_to_prune,amounts_to_prune)\n",
    "    model_name = 'pruned_model_V'+str(i)\n",
    "    print(\"Pruning pruned_model_V\"+str(i))\n",
    "    pruned_model, pruned_trainer = training_pruned_model(pruned_model,'pruned_billsum_model_V'+str(i),tokenized_billsum,tokenizer,data_collator,compute_metrics,20)\n",
    "    # 1. save T5 model and config\n",
    "    pruned_model.save_pretrained(model_name, from_pt=True)\n",
    "    pruned_model.config.save_pretrained(model_name, from_pt=True)\n",
    "    # 2. save model_state_dict (save mask)\n",
    "    torch.save(pruned_model.state_dict(), model_name+'/model_state_dict.pth')\n",
    "    model = pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ceb3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.506624 M\n",
      "0.29887937558704314\n"
     ]
    }
   ],
   "source": [
    "parameters_to_prune = {\n",
    "    'encoder_T5LayerSelfAttention':[], \n",
    "    'encoder_T5LayerFF':[],\n",
    "    'decoder_T5LayerSelfAttention':[], \n",
    "    'decoder_T5LayerCrossAttention':[],\n",
    "    'decoder_T5LayerFF':[],\n",
    "    'lm_head':[],\n",
    "}\n",
    "amounts_to_prune = [\n",
    "        0,\n",
    "        0,\n",
    "        0.1,\n",
    "        0.1,\n",
    "        0.1,\n",
    "        0.2,\n",
    "    ]\n",
    "#model = load_pruned_model('pruned_model_V5')\n",
    "pruned_model = puring(model,parameters_to_prune,amounts_to_prune)\n",
    "model_name = 'pruned_model_V6'\n",
    "pruned_model, pruned_trainer = training_pruned_model(pruned_model,'pruned_billsum_model_V6',tokenized_billsum,tokenizer,data_collator,compute_metrics,40)\n",
    "# 1. save T5 model and config\n",
    "pruned_model.save_pretrained(model_name, from_pt=True)\n",
    "pruned_model.config.save_pretrained(model_name, from_pt=True)\n",
    "# 2. save model_state_dict (save mask)\n",
    "torch.save(pruned_model.state_dict(), model_name+'/model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a8569395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pruned_model_V5 were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.0.SelfAttention.k.weight_mask', 'decoder.block.0.layer.0.SelfAttention.k.weight_orig', 'decoder.block.0.layer.0.SelfAttention.o.weight_mask', 'decoder.block.0.layer.0.SelfAttention.o.weight_orig', 'decoder.block.0.layer.0.SelfAttention.q.weight_mask', 'decoder.block.0.layer.0.SelfAttention.q.weight_orig', 'decoder.block.0.layer.0.SelfAttention.v.weight_mask', 'decoder.block.0.layer.0.SelfAttention.v.weight_orig', 'decoder.block.0.layer.1.EncDecAttention.k.weight_mask', 'decoder.block.0.layer.1.EncDecAttention.k.weight_orig', 'decoder.block.0.layer.1.EncDecAttention.o.weight_mask', 'decoder.block.0.layer.1.EncDecAttention.o.weight_orig', 'decoder.block.0.layer.1.EncDecAttention.q.weight_mask', 'decoder.block.0.layer.1.EncDecAttention.q.weight_orig', 'decoder.block.0.layer.1.EncDecAttention.v.weight_mask', 'decoder.block.0.layer.1.EncDecAttention.v.weight_orig', 'decoder.block.0.layer.2.DenseReluDense.wi.weight_mask', 'decoder.block.0.layer.2.DenseReluDense.wi.weight_orig', 'decoder.block.0.layer.2.DenseReluDense.wo.weight_mask', 'decoder.block.0.layer.2.DenseReluDense.wo.weight_orig', 'decoder.block.1.layer.0.SelfAttention.k.weight_mask', 'decoder.block.1.layer.0.SelfAttention.k.weight_orig', 'decoder.block.1.layer.0.SelfAttention.o.weight_mask', 'decoder.block.1.layer.0.SelfAttention.o.weight_orig', 'decoder.block.1.layer.0.SelfAttention.q.weight_mask', 'decoder.block.1.layer.0.SelfAttention.q.weight_orig', 'decoder.block.1.layer.0.SelfAttention.v.weight_mask', 'decoder.block.1.layer.0.SelfAttention.v.weight_orig', 'decoder.block.1.layer.1.EncDecAttention.k.weight_mask', 'decoder.block.1.layer.1.EncDecAttention.k.weight_orig', 'decoder.block.1.layer.1.EncDecAttention.o.weight_mask', 'decoder.block.1.layer.1.EncDecAttention.o.weight_orig', 'decoder.block.1.layer.1.EncDecAttention.q.weight_mask', 'decoder.block.1.layer.1.EncDecAttention.q.weight_orig', 'decoder.block.1.layer.1.EncDecAttention.v.weight_mask', 'decoder.block.1.layer.1.EncDecAttention.v.weight_orig', 'decoder.block.1.layer.2.DenseReluDense.wi.weight_mask', 'decoder.block.1.layer.2.DenseReluDense.wi.weight_orig', 'decoder.block.1.layer.2.DenseReluDense.wo.weight_mask', 'decoder.block.1.layer.2.DenseReluDense.wo.weight_orig', 'decoder.block.2.layer.0.SelfAttention.k.weight_mask', 'decoder.block.2.layer.0.SelfAttention.k.weight_orig', 'decoder.block.2.layer.0.SelfAttention.o.weight_mask', 'decoder.block.2.layer.0.SelfAttention.o.weight_orig', 'decoder.block.2.layer.0.SelfAttention.q.weight_mask', 'decoder.block.2.layer.0.SelfAttention.q.weight_orig', 'decoder.block.2.layer.0.SelfAttention.v.weight_mask', 'decoder.block.2.layer.0.SelfAttention.v.weight_orig', 'decoder.block.2.layer.1.EncDecAttention.k.weight_mask', 'decoder.block.2.layer.1.EncDecAttention.k.weight_orig', 'decoder.block.2.layer.1.EncDecAttention.o.weight_mask', 'decoder.block.2.layer.1.EncDecAttention.o.weight_orig', 'decoder.block.2.layer.1.EncDecAttention.q.weight_mask', 'decoder.block.2.layer.1.EncDecAttention.q.weight_orig', 'decoder.block.2.layer.1.EncDecAttention.v.weight_mask', 'decoder.block.2.layer.1.EncDecAttention.v.weight_orig', 'decoder.block.2.layer.2.DenseReluDense.wi.weight_mask', 'decoder.block.2.layer.2.DenseReluDense.wi.weight_orig', 'decoder.block.2.layer.2.DenseReluDense.wo.weight_mask', 'decoder.block.2.layer.2.DenseReluDense.wo.weight_orig', 'decoder.block.3.layer.0.SelfAttention.k.weight_mask', 'decoder.block.3.layer.0.SelfAttention.k.weight_orig', 'decoder.block.3.layer.0.SelfAttention.o.weight_mask', 'decoder.block.3.layer.0.SelfAttention.o.weight_orig', 'decoder.block.3.layer.0.SelfAttention.q.weight_mask', 'decoder.block.3.layer.0.SelfAttention.q.weight_orig', 'decoder.block.3.layer.0.SelfAttention.v.weight_mask', 'decoder.block.3.layer.0.SelfAttention.v.weight_orig', 'decoder.block.3.layer.1.EncDecAttention.k.weight_mask', 'decoder.block.3.layer.1.EncDecAttention.k.weight_orig', 'decoder.block.3.layer.1.EncDecAttention.o.weight_mask', 'decoder.block.3.layer.1.EncDecAttention.o.weight_orig', 'decoder.block.3.layer.1.EncDecAttention.q.weight_mask', 'decoder.block.3.layer.1.EncDecAttention.q.weight_orig', 'decoder.block.3.layer.1.EncDecAttention.v.weight_mask', 'decoder.block.3.layer.1.EncDecAttention.v.weight_orig', 'decoder.block.3.layer.2.DenseReluDense.wi.weight_mask', 'decoder.block.3.layer.2.DenseReluDense.wi.weight_orig', 'decoder.block.3.layer.2.DenseReluDense.wo.weight_mask', 'decoder.block.3.layer.2.DenseReluDense.wo.weight_orig', 'decoder.block.4.layer.0.SelfAttention.k.weight_mask', 'decoder.block.4.layer.0.SelfAttention.k.weight_orig', 'decoder.block.4.layer.0.SelfAttention.o.weight_mask', 'decoder.block.4.layer.0.SelfAttention.o.weight_orig', 'decoder.block.4.layer.0.SelfAttention.q.weight_mask', 'decoder.block.4.layer.0.SelfAttention.q.weight_orig', 'decoder.block.4.layer.0.SelfAttention.v.weight_mask', 'decoder.block.4.layer.0.SelfAttention.v.weight_orig', 'decoder.block.4.layer.1.EncDecAttention.k.weight_mask', 'decoder.block.4.layer.1.EncDecAttention.k.weight_orig', 'decoder.block.4.layer.1.EncDecAttention.o.weight_mask', 'decoder.block.4.layer.1.EncDecAttention.o.weight_orig', 'decoder.block.4.layer.1.EncDecAttention.q.weight_mask', 'decoder.block.4.layer.1.EncDecAttention.q.weight_orig', 'decoder.block.4.layer.1.EncDecAttention.v.weight_mask', 'decoder.block.4.layer.1.EncDecAttention.v.weight_orig', 'decoder.block.4.layer.2.DenseReluDense.wi.weight_mask', 'decoder.block.4.layer.2.DenseReluDense.wi.weight_orig', 'decoder.block.4.layer.2.DenseReluDense.wo.weight_mask', 'decoder.block.4.layer.2.DenseReluDense.wo.weight_orig', 'decoder.block.5.layer.0.SelfAttention.k.weight_mask', 'decoder.block.5.layer.0.SelfAttention.k.weight_orig', 'decoder.block.5.layer.0.SelfAttention.o.weight_mask', 'decoder.block.5.layer.0.SelfAttention.o.weight_orig', 'decoder.block.5.layer.0.SelfAttention.q.weight_mask', 'decoder.block.5.layer.0.SelfAttention.q.weight_orig', 'decoder.block.5.layer.0.SelfAttention.v.weight_mask', 'decoder.block.5.layer.0.SelfAttention.v.weight_orig', 'decoder.block.5.layer.1.EncDecAttention.k.weight_mask', 'decoder.block.5.layer.1.EncDecAttention.k.weight_orig', 'decoder.block.5.layer.1.EncDecAttention.o.weight_mask', 'decoder.block.5.layer.1.EncDecAttention.o.weight_orig', 'decoder.block.5.layer.1.EncDecAttention.q.weight_mask', 'decoder.block.5.layer.1.EncDecAttention.q.weight_orig', 'decoder.block.5.layer.1.EncDecAttention.v.weight_mask', 'decoder.block.5.layer.1.EncDecAttention.v.weight_orig', 'decoder.block.5.layer.2.DenseReluDense.wi.weight_mask', 'decoder.block.5.layer.2.DenseReluDense.wi.weight_orig', 'decoder.block.5.layer.2.DenseReluDense.wo.weight_mask', 'decoder.block.5.layer.2.DenseReluDense.wo.weight_orig', 'encoder.block.0.layer.0.SelfAttention.k.weight_mask', 'encoder.block.0.layer.0.SelfAttention.k.weight_orig', 'encoder.block.0.layer.0.SelfAttention.o.weight_mask', 'encoder.block.0.layer.0.SelfAttention.o.weight_orig', 'encoder.block.0.layer.0.SelfAttention.q.weight_mask', 'encoder.block.0.layer.0.SelfAttention.q.weight_orig', 'encoder.block.0.layer.0.SelfAttention.v.weight_mask', 'encoder.block.0.layer.0.SelfAttention.v.weight_orig', 'encoder.block.0.layer.1.DenseReluDense.wi.weight_mask', 'encoder.block.0.layer.1.DenseReluDense.wi.weight_orig', 'encoder.block.0.layer.1.DenseReluDense.wo.weight_mask', 'encoder.block.0.layer.1.DenseReluDense.wo.weight_orig', 'encoder.block.1.layer.0.SelfAttention.k.weight_mask', 'encoder.block.1.layer.0.SelfAttention.k.weight_orig', 'encoder.block.1.layer.0.SelfAttention.o.weight_mask', 'encoder.block.1.layer.0.SelfAttention.o.weight_orig', 'encoder.block.1.layer.0.SelfAttention.q.weight_mask', 'encoder.block.1.layer.0.SelfAttention.q.weight_orig', 'encoder.block.1.layer.0.SelfAttention.v.weight_mask', 'encoder.block.1.layer.0.SelfAttention.v.weight_orig', 'encoder.block.1.layer.1.DenseReluDense.wi.weight_mask', 'encoder.block.1.layer.1.DenseReluDense.wi.weight_orig', 'encoder.block.1.layer.1.DenseReluDense.wo.weight_mask', 'encoder.block.1.layer.1.DenseReluDense.wo.weight_orig', 'encoder.block.2.layer.0.SelfAttention.k.weight_mask', 'encoder.block.2.layer.0.SelfAttention.k.weight_orig', 'encoder.block.2.layer.0.SelfAttention.o.weight_mask', 'encoder.block.2.layer.0.SelfAttention.o.weight_orig', 'encoder.block.2.layer.0.SelfAttention.q.weight_mask', 'encoder.block.2.layer.0.SelfAttention.q.weight_orig', 'encoder.block.2.layer.0.SelfAttention.v.weight_mask', 'encoder.block.2.layer.0.SelfAttention.v.weight_orig', 'encoder.block.2.layer.1.DenseReluDense.wi.weight_mask', 'encoder.block.2.layer.1.DenseReluDense.wi.weight_orig', 'encoder.block.2.layer.1.DenseReluDense.wo.weight_mask', 'encoder.block.2.layer.1.DenseReluDense.wo.weight_orig', 'encoder.block.3.layer.0.SelfAttention.k.weight_mask', 'encoder.block.3.layer.0.SelfAttention.k.weight_orig', 'encoder.block.3.layer.0.SelfAttention.o.weight_mask', 'encoder.block.3.layer.0.SelfAttention.o.weight_orig', 'encoder.block.3.layer.0.SelfAttention.q.weight_mask', 'encoder.block.3.layer.0.SelfAttention.q.weight_orig', 'encoder.block.3.layer.0.SelfAttention.v.weight_mask', 'encoder.block.3.layer.0.SelfAttention.v.weight_orig', 'encoder.block.3.layer.1.DenseReluDense.wi.weight_mask', 'encoder.block.3.layer.1.DenseReluDense.wi.weight_orig', 'encoder.block.3.layer.1.DenseReluDense.wo.weight_mask', 'encoder.block.3.layer.1.DenseReluDense.wo.weight_orig', 'encoder.block.4.layer.0.SelfAttention.k.weight_mask', 'encoder.block.4.layer.0.SelfAttention.k.weight_orig', 'encoder.block.4.layer.0.SelfAttention.o.weight_mask', 'encoder.block.4.layer.0.SelfAttention.o.weight_orig', 'encoder.block.4.layer.0.SelfAttention.q.weight_mask', 'encoder.block.4.layer.0.SelfAttention.q.weight_orig', 'encoder.block.4.layer.0.SelfAttention.v.weight_mask', 'encoder.block.4.layer.0.SelfAttention.v.weight_orig', 'encoder.block.4.layer.1.DenseReluDense.wi.weight_mask', 'encoder.block.4.layer.1.DenseReluDense.wi.weight_orig', 'encoder.block.4.layer.1.DenseReluDense.wo.weight_mask', 'encoder.block.4.layer.1.DenseReluDense.wo.weight_orig', 'encoder.block.5.layer.0.SelfAttention.k.weight_mask', 'encoder.block.5.layer.0.SelfAttention.k.weight_orig', 'encoder.block.5.layer.0.SelfAttention.o.weight_mask', 'encoder.block.5.layer.0.SelfAttention.o.weight_orig', 'encoder.block.5.layer.0.SelfAttention.q.weight_mask', 'encoder.block.5.layer.0.SelfAttention.q.weight_orig', 'encoder.block.5.layer.0.SelfAttention.v.weight_mask', 'encoder.block.5.layer.0.SelfAttention.v.weight_orig', 'encoder.block.5.layer.1.DenseReluDense.wi.weight_mask', 'encoder.block.5.layer.1.DenseReluDense.wi.weight_orig', 'encoder.block.5.layer.1.DenseReluDense.wo.weight_mask', 'encoder.block.5.layer.1.DenseReluDense.wo.weight_orig', 'lm_head.weight_mask']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at pruned_model_V5 and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.506624 M\n",
      "0.29938692332264316\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'pruned_model_V6'\n",
    "pruned_model = load_pruned_model(checkpoint)\n",
    "show_param_ratio(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6ecb7-bc87-4233-99b0-bb45db24c9a4",
   "metadata": {},
   "source": [
    "## Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99afdcf-c099-4eef-95cb-8870d5bfde2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 32/32 [00:00<00:00, 1056.34 examples/s]\n",
      "Map: 100%|██████████| 3237/3237 [00:01<00:00, 2105.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "billsum_test = load_dataset(\"billsum\", split=\"test\")\n",
    "tokenized_billsum_test = billsum_test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d6e64066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.982614517211914,\n",
       " 'eval_rouge1': 0.1934,\n",
       " 'eval_rouge2': 0.0898,\n",
       " 'eval_rougeL': 0.1611,\n",
       " 'eval_rougeLsum': 0.1608,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 4.4159,\n",
       " 'eval_samples_per_second': 7.247,\n",
       " 'eval_steps_per_second': 3.623,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_trainer.evaluate(tokenized_billsum_test['train'],max_length = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "18b10f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.0735673904418945,\n",
       " 'eval_rouge1': 0.3314,\n",
       " 'eval_rouge2': 0.1116,\n",
       " 'eval_rougeL': 0.2501,\n",
       " 'eval_rougeLsum': 0.2496,\n",
       " 'eval_gen_len': 48.875,\n",
       " 'eval_runtime': 7.9999,\n",
       " 'eval_samples_per_second': 4.0,\n",
       " 'eval_steps_per_second': 2.0}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_trainer.evaluate(tokenized_billsum_test['train'],max_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5132e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.0735673904418945,\n",
       " 'eval_rouge1': 0.3312,\n",
       " 'eval_rouge2': 0.1059,\n",
       " 'eval_rougeL': 0.2434,\n",
       " 'eval_rougeLsum': 0.2436,\n",
       " 'eval_gen_len': 68.25,\n",
       " 'eval_runtime': 10.1828,\n",
       " 'eval_samples_per_second': 3.143,\n",
       " 'eval_steps_per_second': 1.571}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_trainer.evaluate(tokenized_billsum_test['train'],max_length = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ff7dd707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 17:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.0735673904418945,\n",
       " 'eval_rouge1': 0.2946,\n",
       " 'eval_rouge2': 0.0913,\n",
       " 'eval_rougeL': 0.2288,\n",
       " 'eval_rougeLsum': 0.2294,\n",
       " 'eval_gen_len': 97.3125,\n",
       " 'eval_runtime': 10.4055,\n",
       " 'eval_samples_per_second': 3.075,\n",
       " 'eval_steps_per_second': 1.538}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_trainer.evaluate(tokenized_billsum_test['train'],max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5cd3292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.0735673904418945,\n",
       " 'eval_rouge1': 0.2624,\n",
       " 'eval_rouge2': 0.0801,\n",
       " 'eval_rougeL': 0.2106,\n",
       " 'eval_rougeLsum': 0.2108,\n",
       " 'eval_gen_len': 142.0938,\n",
       " 'eval_runtime': 13.9898,\n",
       " 'eval_samples_per_second': 2.287,\n",
       " 'eval_steps_per_second': 1.144}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_trainer.evaluate(tokenized_billsum_test['train'],max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "60c0c267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.0735673904418945,\n",
       " 'eval_rouge1': 0.2497,\n",
       " 'eval_rouge2': 0.0757,\n",
       " 'eval_rougeL': 0.2006,\n",
       " 'eval_rougeLsum': 0.2007,\n",
       " 'eval_gen_len': 170.2188,\n",
       " 'eval_runtime': 16.8414,\n",
       " 'eval_samples_per_second': 1.9,\n",
       " 'eval_steps_per_second': 0.95}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_trainer.evaluate(tokenized_billsum_test['train'],max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83c8b8-bb2e-4f6c-8d67-f53d625fe1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pruned_trainer.predict(tokenized_billsum_test['train'],max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a78d7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.where(results[0] != -100, results[0], tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cbfd5821-7571-47b7-a3f3-adae1a123831",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_prediction = tokenizer.batch_decode(results, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "047ff645-68ef-41be-b64d-dff82b0d661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c8c4da8f-16d0-4ec3-9018-44334504b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                                            Predict\n",
      "0    0  Transportation Security Act of 2007 - Amends t...\n",
      "1    1  Refuges Act of 2016 This bill amends the Immig...\n",
      "2    2  Foster Foster Foster Foster Act - Directs the ...\n",
      "3    3  Directs the Secretary of Defense to establish ...\n",
      "4    4  Government Government Accountability Act - Req...\n",
      "5    5  National Coin Coin Act - Directs the Secretary...\n",
      "6    6  National Housing Housing Act of 2013 - Directs...\n",
      "7    7  Directs the Attorney General to establish a pr...\n",
      "8    8  Micro Microuse Act of 2001 - Amends the Small ...\n",
      "9    9  Private Private Private Privat Privat Privatiz...\n",
      "10  10  Cyber Cyber Cyber Cyber Cyber Cyber Cyber Cybe...\n",
      "11  11  Amends the Internal Revenue Code to establish ...\n",
      "12  12  National Neuro Neuro Neuroforensicforensicfore...\n",
      "13  13  Amends the Internal Revenue Code to allow a dr...\n",
      "14  14  Energy Energy Energy Act of 2007 - Amends the ...\n",
      "15  15  Fairness Act - Directs the Attorney General to...\n",
      "16  16  Insurance Insurance Insurance Act of 2001 - Am...\n",
      "17  17  Social Security Act - Directs the Secretary of...\n",
      "18  18  Environmental Protection Act - Directs the Sec...\n",
      "19  19  Amends the Internal Revenue Code to establish ...\n",
      "20  20  Foodfoodfoodfoodfoods Act of 2001 - Directs th...\n",
      "21  21  Medicare Medicare Access Access Act of 2016 Th...\n",
      "22  22  Directs the Secretary of the Interior to estab...\n",
      "23  23  Rural Rural Development Act of 2005 - Amends t...\n",
      "24  24  Homeowners Act of 2005 - Amends the Internal R...\n",
      "25  25  Medicare Medicare Advantage Act of 2015 This b...\n",
      "26  26  Directs the Secretary of Agriculture to establ...\n",
      "27  27  Water Water Protection Act of 2002 - Amends th...\n",
      "28  28  Immigration and Immigration and Immigration Ac...\n",
      "29  29  Federal Reserve Reserve Act of 2010 - Directs ...\n",
      "30  30  National Park Park. National Park. National Pa...\n",
      "31  31  Lands Act of 2007 - Directs the Secretary of t...\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['ID','Predict'])\n",
    "\n",
    "for i, prediction in enumerate(decoded_prediction):\n",
    "    # Escape quotes by replacing \",\" with \".\"\n",
    "    summary_escaped = prediction.replace(',', '.')\n",
    "    \n",
    "    # Create a new row DataFrame and append it\n",
    "    new_row = pd.DataFrame({'ID': [i], 'Predict': [summary_escaped]})\n",
    "    df_results = pd.concat([df_results, new_row], ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "603fcc88-75b8-4f70-8c03-fe6d7a0dc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to escape double quotes and handle newlines\n",
    "def escape_special_characters(text):\n",
    "    return text.replace('\"', '\"\"').replace('\\n', ' ')\n",
    "\n",
    "# Apply escaping to the 'Summary' column\n",
    "df_results['Predict'] = df_results['Predict'].apply(escape_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f427e-58eb-4edf-b6dc-dbe95c136b2c",
   "metadata": {},
   "source": [
    "### Dump Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b574eded-d30d-4a4e-971e-94f97d7a6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('test.csv', index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "#df_results = pd.read_csv('test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a28d7-4b4f-48d1-8002-a44e15a52de2",
   "metadata": {},
   "source": [
    "### Calculating ROUGE-Lsum with build-in Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2ae31199-f959-4cf0-94e0-0446a5c1b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lcs(X, Y):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the longest common subsequence of sequences X and Y.\n",
    "    \"\"\"\n",
    "    m, n = len(X), len(Y)\n",
    "    L = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if X[i - 1] == Y[j - 1]:\n",
    "                L[i][j] = L[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "\n",
    "    return L[m][n]\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the ROUGE-Lsum score based on the longest common subsequence summed over all sentences in the summaries.\n",
    "    \n",
    "    Args:\n",
    "    solution (pd.DataFrame): The DataFrame containing the correct summaries.\n",
    "    submission (pd.DataFrame): The DataFrame containing participant's predicted summaries.\n",
    "    row_id_column_name (str): The column name for the row ID in both DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    float: The mean ROUGE-Lsum score across all predictions.\n",
    "    \"\"\"\n",
    "    # Ensure indices for proper alignment\n",
    "    solution.set_index(row_id_column_name, inplace=True)\n",
    "    submission.set_index(row_id_column_name, inplace=True)\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    for idx in solution.index:\n",
    "        if idx not in submission.index:\n",
    "            raise ParticipantVisibleError(f\"Missing prediction for ID {idx}.\")\n",
    "\n",
    "        ref_summary = solution.loc[idx, 'Label']\n",
    "        pred_summary = submission.loc[idx, 'Predict']\n",
    "\n",
    "        # Tokenize sentences\n",
    "        ref_sentences = ref_summary.split('.')\n",
    "        pred_sentences = pred_summary.split('.')\n",
    "\n",
    "        # Calculate LCS for each sentence pair\n",
    "        lcs_sum = 0\n",
    "        for ref_sent in ref_sentences:\n",
    "            ref_tokens = ref_sent.strip().lower().split()\n",
    "            best_lcs = 0\n",
    "            for pred_sent in pred_sentences:\n",
    "                pred_tokens = pred_sent.strip().lower().split()\n",
    "                lcs_length = calculate_lcs(ref_tokens, pred_tokens)\n",
    "                best_lcs = max(best_lcs, lcs_length)\n",
    "            lcs_sum += best_lcs\n",
    "\n",
    "        # Calculate ROUGE-L for the current pair of summaries\n",
    "        ref_length = sum(len(sent.strip().split()) for sent in ref_sentences)\n",
    "        if ref_length > 0:\n",
    "            rouge_l = lcs_sum / ref_length\n",
    "        else:\n",
    "            rouge_l = 0\n",
    "        total_score += rouge_l\n",
    "\n",
    "    # Compute the average ROUGE-L score across all submissions\n",
    "    mean_rouge_lsum = total_score / len(solution)\n",
    "\n",
    "    return mean_rouge_lsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8a9c1f39-e16e-483b-9b63-e87ce6ab32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                                              Label\n",
      "0    0  Securing America's Facilities. Equipment and R...\n",
      "1    1  Liberian Refugee Immigration Fairness Act of 2...\n",
      "2    2  Every Child Deserves a Family Act - Prohibits ...\n",
      "3    3  Directs the United States to: (1) take all nec...\n",
      "4    4  Spending Reduction Act - Requires the head of ...\n",
      "5    5  National Purple Heart Hall of Honor Commemorat...\n",
      "6    6  Eleanor Smith Inclusive Home Design Act of 201...\n",
      "7    7  Directs the Attorney General to:  (1) conduct ...\n",
      "8    8  Amends the Riegle Community Development and Re...\n",
      "9    9  Protection of Homes. Small Businesses. and Pri...\n",
      "10  10  Seniors Financial Fraud Prevention Act of 2010...\n",
      "11  11  Competitive and Open Markets That Protect and ...\n",
      "12  12  National Neurological Diseases Surveillance Sy...\n",
      "13  13  Deadly Driver Reduction Act - Amends Federal t...\n",
      "14  14  Recovery Through Building Renovation Act of 20...\n",
      "15  15  Data Broker Accountability and Transparency Ac...\n",
      "16  16  Taxpayer Savings and Employee Notification Act...\n",
      "17  17  Social Security Safety Dividend Act - Directs ...\n",
      "18  18  Department of Environmental Protection Act - R...\n",
      "19  19  Indian Trust Asset and Trust Fund Management a...\n",
      "20  20  Directs the Secretary of Agriculture to establ...\n",
      "21  21  Medicare Common Access Card Act of 2017 This b...\n",
      "22  22  Limits Federal assistance to the Northern Mari...\n",
      "23  23  Agricultural Producers Value-Added Investment ...\n",
      "24  24  Home Ownership Vesting Plan Act of 2009 - Amen...\n",
      "25  25  Establishing Beneficiary Equity in the Hospita...\n",
      "26  26  Nutria Eradication and Control Act of 2009 - (...\n",
      "27  27  Clean Water Infrastructure Financing Act of 20...\n",
      "28  28  Citizenship Promotion Act of 1996 - Amends the...\n",
      "29  29  Dollar Bill Act of 2013 - Directs the Board of...\n",
      "30  30  Cesar Chavez National Historical Park Act This...\n",
      "31  31  Canyon Mountain Land Conveyance Act of 2013 - ...\n"
     ]
    }
   ],
   "source": [
    "df_label = pd.DataFrame(columns=['ID','Label'])\n",
    "\n",
    "for i, label in enumerate(billsum_test['train']):\n",
    "    # Escape quotes by replacing \",\" with \".\"\n",
    "    label_escaped = label['summary'].replace(',', '.')\n",
    "    \n",
    "    # Create a new row DataFrame and append it\n",
    "    new_row = pd.DataFrame({'ID': [i], 'Label': [label_escaped]})\n",
    "    df_label = pd.concat([df_label, new_row], ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3b5a94ba-23af-4849-a105-798182fe9e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2400331001863319"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(df_label, df_results, 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c9b499d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2400331001863319"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(df_label, df_results, 'ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
